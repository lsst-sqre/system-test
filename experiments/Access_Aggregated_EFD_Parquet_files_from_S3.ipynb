{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acessing EFD aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angelo Fausti, Simon Krughoff\n",
    "\n",
    "August 10, 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we show how to access data produced by the EFD aggregator. This was done as part of an internal demonstration of the EFD aggregator and used the EFD Sandox instance for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EFD aggregated streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EFD [aggregator](https://kafka-aggregator.lsst.io) is responsible for consuming the EFD data streams and produce a new set of aggregated streams which are then converted to Parquet files, partioned by time, and stored in an object store.\n",
    "\n",
    "To demonstrate the EFD aggregator we used the [aggregator example module](https://kafka-aggregator.lsst.io/configuration.html#example-module-configuration). \n",
    "\n",
    "In this experiment we initialize ten “example topics” and produce messages for them at 10Hz. For each field in the source topic the aggregator adds the following summary statistics `min`, `mean`, `median`, `stdev`, `max` and aggregate the messages in windows of 1s. See the the [aggregator configuration settings](https://kafka-aggregator.lsst.io/configuration.html#configuration-settings) for more details.\n",
    "\n",
    "A new set of aggretated topics is created in Kafka and we use the Kafka [S3 Sink Connector](https://docs.confluent.io/current/connect/kafka-connect-s3/) to write the data into Parquet files, in this example to Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading EFD Parquet files from S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"efd-sandbox.data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S3 credentials can be added to `~/.aws/credentials` file. They are stored in SQuaRE 1Password. Search for EFD AWS S3 credentials. The S3 region can be added to the `~/.aws/config` file.\n",
    "\n",
    "For example:\n",
    "```\n",
    "cat ~/.aws/credentials\n",
    "[default]\n",
    "aws_access_key_id = <the aws_access_key_id>\n",
    "aws_secret_access_key = <the aws_secret_access_key>\n",
    "```\n",
    "and\n",
    "```\n",
    "cat ~/.aws/config \n",
    "[default]\n",
    "region=us-east-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the S3 Sink connector is configured to partition data by time on an hourly basis. The following helps to construct the path to find the Parquet files on S3 for one of the aggregated topics, in this example, `example-002-aggregated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"example-002-aggregated\"\n",
    "year = \"2020\"\n",
    "month = \"08\"\n",
    "day = \"07\"\n",
    "hour = \"22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `bucket.download_fileobj()` method to download the Parquet files into a buffer, and then Pyarrow to read the files, convert and append them to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for obj in bucket.objects.filter(Prefix=f\"topics/{topic}/year={year}/month={month}/day={day}/hour={hour}\"):\n",
    "    buffer = io.BytesIO()\n",
    "    bucket.download_fileobj(obj.key, buffer)\n",
    "    df = df.append(pq.read_table(buffer).to_pandas())\n",
    "    print(f\"{bucket.name}:{obj.key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S3 Sink connector is configured to invoke file commits to S3 every 10 minutes (see the `rotate_interval_ms` configuration setting) that's why you see 6 files in this path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the aggregated stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df.plot(x='time', y='mean_value1', c='white', figsize=(15,5))\n",
    "p.fill_between(x='time', y1='min_value1', y2='max_value1', data=df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
