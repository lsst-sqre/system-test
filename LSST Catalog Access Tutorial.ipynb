{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSST Catalog Access Tutorial\n",
    "\n",
    "This notebook provides a simple tutorial for how to access the catalog data for LSST.\n",
    "\n",
    "We use the IVOA TAP (Table Access Procotol) standard to access the catalog data.  This standardizes usage so that 3rd party clients and astronomers can easily move their queries around between datasets, and discover what data is available for them to query.\n",
    "\n",
    "Here's a link to the standard: http://www.ivoa.net/documents/TAP/\n",
    "\n",
    "The TAP service uses a query language similar to SQL called ADQL.  For the specifics about the syntax and keywords, look at: http://www.ivoa.net/documents/latest/ADQL.html\n",
    "\n",
    "### TOPCAT usage\n",
    "\n",
    "Some astronomers prefer to use tools like TOPCAT to access the catalogs.  In order to connect TOPCAT to the TAP service, you will need to provide a service endpoint for TOPCAT to use.  Hou can find the service endpoint for the environment you are using by executing the cell in [Section 1](#1.-Create-the-client).  If you can contact the endpoint printed by that cell from your network, you can also use this as the \"Selected TAP Service\" in TOPCAT.\n",
    "\n",
    "See [this](https://nb.lsst.io/environment/tokens.html) page for more in formation on using the TAP service with TOPCAT.  Note that those instructions assume a TAP service running at NCSA, so be sure to substitute the endopint for your specific environment as provided below for the NCSA specific one in the documentation.\n",
    "\n",
    "### Portal usage\n",
    "\n",
    "The TAP service is used by the portal and users can use the Portal UI to help them construct queries.\n",
    "\n",
    "### Notebook Usage\n",
    "\n",
    "In the following steps, we will create a client to query the catalog, show how you can discover what tables and columns exist in the catalog, as well as how to run queries and process their results using python.\n",
    "\n",
    "For this example, we will be using pyvo: https://pyvo.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the client\n",
    "\n",
    "First, you must retrieve the TAP service object to use for querying the data.  This will be the object you call to run queries on.  Run the cell below to create your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from rubin_jupyter_utils.lab.notebook import get_tap_service, retrieve_query\n",
    "    \n",
    "pandas.set_option('display.max_rows', 1000)\n",
    "service = get_tap_service()\n",
    "\n",
    "md(f'The service endpoint for TAP in this environment is:\\n\\n &#10145;&nbsp;&nbsp;   {service.baseurl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Schema discovery\n",
    "\n",
    "Now that you've connected to the TAP service, you need to know the names of schemas, tables, and columns (and their datatypes) to be able to write queries to access the data.\n",
    "\n",
    "Luckily, TAP provides for a standard way of discovering what schemas, tables, and columns are available.  This information is stored in a database named TAP_SCHEMA.\n",
    "\n",
    "### 2.1 What schemas can I query?\n",
    "\n",
    "In order to discover what database schemas are being served and can be queried against, you can run the following cell.  You should see a numpy compatible table printed out in the notebook that has two important columns, description and schema_name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.search(\"SELECT schema_name, description FROM TAP_SCHEMA.schemas\")\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TAP_SCHEMA.schemas** behaves like a normal SQL table, with mostly the same query language. Just like standard SQL, you can also retrieve all the columns by using the following example.\n",
    "\n",
    "**TIP:** Calling to_table() on a result will give you an AstroPy table (http://docs.astropy.org/en/stable/_modules/astropy/table/table.html).\n",
    "\n",
    "From an astropy table, you can also convert to a pandas dataframe by using to_pandas()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.search(\"SELECT * FROM TAP_SCHEMA.schemas\")\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 What tables can I query?\n",
    "\n",
    "Like the TAP_SCHEMA.schemas table, there is also a **TAP_SCHEMA.tables** table, which contains the names of the tables, and which schema each table belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.search(\"SELECT * FROM TAP_SCHEMA.tables\")\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you may want to act programmatically on the results.  There are also functions you can call on the results object to get the columns / fields, and iterate through the rows, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column names for TAP_SCHEMA.tables are:\")\n",
    "for field in results.fieldnames:\n",
    "    print(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the column names to limit the columns returned by the query, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.search(\"SELECT schema_name, table_name FROM TAP_SCHEMA.tables\")\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 What columns are in a table, and what datatype is each column?\n",
    "\n",
    "Now that we've learned what tables exist, we're close to querying for real catalog data.  But generally, we want to limit the number of columns to return to exactly what we need to get the job done.  This generally makes things faster and better for everyone.  While you can get all the columns that exist (with a \"SELECT * from TAP_SCHEMA.columns\"), this will be over all the tables in all the schemas, so normally we want to limit the query to a particular table.\n",
    "\n",
    "Let's discover what columns exist in the wise_00.allwise_p3as_mep table using the following code.\n",
    "\n",
    "**TIP**: Be careful with your quoting, since some quotes are processed by python, and some quotes are passed down as a part of the query.  My suggestion is to use double quotes for python, and single quotes for SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.search(\"SELECT * from TAP_SCHEMA.columns WHERE table_name = 'wise_01.allwise_p3as_mep'\")\n",
    "\n",
    "print(\"Column names for TAP_SCHEMA.columns are:\")\n",
    "print(results.fieldnames)\n",
    "\n",
    "print(\"Columns that exist in wise_01.allwise_p3as_mep are:\")\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most useful columns in TAP_SCHEMA.columns are the column_name, unit, and description.  Let's just query those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.search(\"SELECT column_name, description, unit FROM TAP_SCHEMA.columns WHERE table_name = 'wise_01.allwise_p3as_mep'\")\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example queries\n",
    "\n",
    "Here are some example queries against the wise dataset.  As more queries are supported, they will be added here.\n",
    "\n",
    "**Note**: Not all of these are against the same table, or against the same columns.  These queries are simply for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cone searches\n",
    "\n",
    "How to do a cone search (with a center RA/DEC, and a radius):\n",
    "\n",
    "The format is CONTAINS(POINT('ICRS', dec column or number, ra column or number), CIRCLE('ICRS', center RA, center DEC, radius)) = 1.  All units are in degrees.\n",
    "\n",
    "**TIP**: You can limit the number of rows returned by using maxrec=x as a parameter.  This means it will only return x rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.search(\"SELECT ra, dec FROM wise_01.allwise_p3as_mep WHERE CONTAINS(POINT('ICRS', ra, dec), CIRCLE('ICRS', 1.0, -1.0, .1)) = 1\", maxrec=10)\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Polygon searches\n",
    "\n",
    "Sometimes cone search isn't enough.  Sometimes you want to submit a set of arbitrary vertices and draw your own polygon to limit to.  You can use the POLYGON function.  This behaves like CIRCLE, but allows for any number of RA/DEC coordinate pairs to be vertices.  The polygon is automatically closed between the last vertex and the first vertex.\n",
    "\n",
    "The format is CONTAINS(POINT('ICRS', dec column or number, ra column or number), POLYGON('ICRS', RA 1, DEC 1, RA 2, DEC 2, ...)) = 1. All units are in degrees.\n",
    "\n",
    "Following is an example of using POLYGON to make a square like shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.search(\"SELECT ra, dec FROM wise_01.allwise_p3as_mep WHERE CONTAINS(POINT('ICRS', ra, dec), POLYGON('ICRS', .9, .9, 1, .9, 1, 1, .9, 1)) = 1\", maxrec=10)\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Retrieve a light curve for a source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.search(\"SELECT mjd, w1mpro_ep, w1sigmpro_ep, w2mpro_ep, w2sigmpro_ep FROM wise_01.allwise_p3as_mep WHERE cntr_mf = 2813003101351041856 AND mjd > 55450\")\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can convert the data to a pandas dataframe, and graph it using Bokeh (https://bokeh.pydata.org/en/latest/docs/user_guide.html).\n",
    "\n",
    "**Tip** Note some of the columns don't have data.  You always have to watch out for NULLs or missing data in your code.  This can be really tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import ColumnDataSource, Whisker\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.resources import INLINE\n",
    "try:\n",
    "    from jupyterlabutils import show_with_bokeh_server\n",
    "except ModuleNotFoundError:\n",
    "    from rubin_jupyter_utils.lab import show_with_bokeh_server\n",
    "    \n",
    "output_notebook(INLINE, hide_banner=True)\n",
    "\n",
    "dataframe = results.to_table().to_pandas()\n",
    "\n",
    "# Let's calculate the error bars.\n",
    "dataframe['w1mpro_err_min'] = dataframe['w1mpro_ep'] - dataframe['w1sigmpro_ep']\n",
    "dataframe['w1mpro_err_max'] = dataframe['w1mpro_ep'] + dataframe['w1sigmpro_ep']\n",
    "dataframe['w2mpro_err_min'] = dataframe['w2mpro_ep'] - dataframe['w2sigmpro_ep']\n",
    "dataframe['w2mpro_err_max'] = dataframe['w2mpro_ep'] + dataframe['w2sigmpro_ep']\n",
    "\n",
    "datasource = ColumnDataSource(dataframe)\n",
    "\n",
    "# Make a plot of a certain size\n",
    "p = figure(plot_width=800, plot_height=800)\n",
    "p.xaxis.axis_label = 'Modified Julian date of the mid-point of the observation of the frame'\n",
    "p.yaxis.axis_label = 'Single-exposure profile-fit magnitude'\n",
    "\n",
    "# Let's add the error bars now\n",
    "p.add_layout(Whisker(source=datasource, base='mjd', upper='w1mpro_err_max', lower='w1mpro_err_min'))\n",
    "p.add_layout(Whisker(source=datasource, base='mjd', upper='w2mpro_err_max', lower='w2mpro_err_min'))\n",
    "\n",
    "# Add a circle renderer with a size, color, and alpha.\n",
    "# x and y control which columns to use from the dataframe.\n",
    "p.circle(x='mjd', y='w1mpro_ep', source=datasource, size=5, color=\"navy\")\n",
    "p.circle(x='mjd', y='w2mpro_ep', source=datasource, size=5, color=\"green\")\n",
    "\n",
    "# show the results\n",
    "show_with_bokeh_server(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Asynchronous queries\n",
    "\n",
    "So far, all the queries you have run are called a synchronous queries.  This means that it will continue executing in the notebook until it is finished.  You can see when a Jupyter cell is running because it will have an asterisk to the left of the running cell.  When the query is finished executing and results are returned, the asterisk will become a number.  This is great for short queries that may take seconds or a minute.\n",
    "\n",
    "For longer queries, or running multiple queries at the same time, you may want to consider running an asynchronous query.  Asynchronous queries can be started and allow you to execute more python while the query runs on the database.  Results can be retrieved later on.  This is especially important for queries that are long or may return a lot of results.\n",
    "\n",
    "### 4.1 Asynchronous query example\n",
    "Let's run one of these queries asynchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, submit the job.  This creates it but doesn't run the query yet.\n",
    "job = service.submit_job(\"SELECT schema_name, description FROM TAP_SCHEMA.schemas\")\n",
    "\n",
    "# Here's the URL representing your query.  This is what you need to retrieve\n",
    "# the data again.\n",
    "print('Job URL is', job.url)\n",
    "\n",
    "# Here you can see the job's phase, which is PENDING.\n",
    "print('Job phase is', job.phase)\n",
    "\n",
    "# Now, run the query.\n",
    "job.run()\n",
    "\n",
    "# Here, you can tell python to wait for the job to finish if you don't want to\n",
    "# run anything else while you are waiting.  Then we print out the final state.\n",
    "job.wait(phases=['COMPLETED', 'ERROR'])\n",
    "print('Job phase is', job.phase)\n",
    "\n",
    "# Here's a helpful function that will raise an exception if\n",
    "# your query had an unfortunate incident.\n",
    "job.raise_if_error()\n",
    "\n",
    "# Once it completes successfully, you can work with the results\n",
    "# the same exact way as if it were synchronous. \n",
    "results = job.fetch_result()\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Retrieving results from a previous query\n",
    "\n",
    "If you have already run a query, the reuslts may still be available.  You can easily retrieve the results from a previous query again if you know the URL of the query.\n",
    "\n",
    "You can use the query URL to do a few things:\n",
    "1. Use the File -> Open Query ID and put in the URL to start a notebook to load your results.\n",
    "2. Use TOPCAT TAP Query tool to load the query results into TOPCAT.  Use the \"Resume Job\" tab in the TAP Service tool.\n",
    "3. Use the retrieve_query function from jupyterhubutils.notebook to retrieve the results.\n",
    "\n",
    "Here we will show you the last one to load the same query results as the query we just ran above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_job = retrieve_query(job.url)\n",
    "results = retrieved_job.fetch_result()\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Deleting results\n",
    "\n",
    "Once your asynchronous job is finished, the results are stored on the server until you are ready to retrieve them, as shown in the above section.  When you are done with your results, you can delete them yourself, or the server will delete results automatically after a period of time.  After a job is deleted, you will have to re-run the original query on the server to get the results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Long running queries running as asynchronous\n",
    "\n",
    "Even if you don't have something else to do in the background, sometimes you will want to run queries that take a long time asynchronously.  This is because it is safer from network outages and timeouts to run it that way.  Luckily, there's a helper function to run a query asynchronously and wait for it to finish, just like a synchronous query.  It basically does what we've done above.  run_async will also automatically delete the results after they are received.\n",
    "\n",
    "Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = service.run_async(\"SELECT schema_name, description FROM TAP_SCHEMA.schemas\")\n",
    "results.to_table().show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Future work / Known issues\n",
    "\n",
    "The work for this service is ongoing, and there will be known limitations and bugs.  If you want to contact someone for help, please go to the #dm-dax channel on the LSST Slack or email Christine Banek (cbanek@lsst.org).  You can also file an issue in JIRA and assign it to cbanek.\n",
    "\n",
    "### 5.1 Known limitations\n",
    "\n",
    "This TAP layer is built on top of QServ, which has its own query limitations that may come up.  Here is a document outlining those restrictions:\n",
    "\n",
    "Some ADQL functions are not yet supported:\n",
    "\n",
    "- AREA\n",
    "- BOX\n",
    "- COORDSYS\n",
    "- COORD1\n",
    "- COORD2\n",
    "- INTERSECTS\n",
    "\n",
    "The only coordinate system currently supported is 'ICRS'.\n",
    "\n",
    "https://github.com/lsst/qserv/blob/master/UserManual.md\n",
    "\n",
    "### 5.2 Performance\n",
    "\n",
    "If your query takes more than a minute or so, it's likely you have run a query that is enacting a full table scan.  This can take a long time (~ hour) to return.  Please be patient and don't re-run your query as this may put the database under additional load."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
