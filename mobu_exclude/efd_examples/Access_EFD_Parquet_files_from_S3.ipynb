{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acessing EFD Parquet files from MinIO S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angelo Fausti, Simon Krughoff\n",
    "\n",
    "November 1, 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The EFD uses the [Amazon S3 Sink connector](https://docs.confluent.io/kafka-connect-s3-sink/current/overview.html) to convert data from Kafka topics from Avro format to Parquet format with snappy compression. \n",
    "In this notebook we show how to access EFD data stored as Parquet files from the MinIO S3-compatible object storage at NCSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Parquet files from MinIO S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon S3 Sink connector runs at LDF, EFD data is replicated from the Summit and stored in an S3 bucket: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"efd-stable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MinIO credentials to read this bucket can be found in 1Password (search for \"EFD MinIO credentials\" and then \"EFD Reader\"). \n",
    "Add the read credentials to the `~/.aws/credentials` file.\n",
    "\n",
    "For example:\n",
    "```\n",
    "cat ~/.aws/credentials\n",
    "[default]\n",
    "aws_access_key_id = <the aws_access_key_id>\n",
    "aws_secret_access_key = <the aws_secret_access_key>\n",
    "```\n",
    "\n",
    "The S3 region can be added to the `~/.aws/config` file. Any value should work with MinIO.\n",
    "\n",
    "```\n",
    "cat ~/.aws/config \n",
    "[default]\n",
    "region=us-east-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',  endpoint_url=\"http://lsst-nfs.ncsa.illinois.edu:9003\")\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate over othe objects use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in bucket.objects.all():\n",
    "    if \"lsst.sal.ATAOS.logevent_heartbeat\" in obj.key:\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connector  is configured to partition data from Kafka topics by time on an hourly basis using the `Record` timestamp (added by Kafka when the message arrived the kafka broker) as reference. \n",
    "The following helps to construct the path to find the Parquet files on S3 for one of the topics, for example, `lsst.sal.ATAOS.logevent_heartbeat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"lsst.sal.ATAOS.logevent_heartbeat\"\n",
    "year = \"2021\"\n",
    "month = \"11\"\n",
    "day = \"08\"\n",
    "hour = \"21\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `bucket.download_fileobj()` method to download the Parquet files into a buffer, and then Pyarrow to read the files, convert and concatenate them into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for obj in bucket.objects.filter(Prefix=f\"topics/{topic}/year={year}/month={month}/day={day}/hour={hour}\"):\n",
    "    buffer = io.BytesIO()\n",
    "    bucket.download_fileobj(obj.key, buffer)\n",
    "    df = pd.concat([df, pq.read_table(buffer).to_pandas()])\n",
    "    print(f\"{bucket.name}:{obj.key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connector is configured to invoke file commits to S3 every 10 minutes (see the [`rotate.interval.ms` configuration setting](https://docs.confluent.io/kafka-connect-s3-sink/current/configuration_options.html)), so you should see 6 files in this path.\n",
    "\n",
    "NOTE: To read all the Parquet files on a given day you can filter the bucket objects using a prefix like `f\"topics/{topic}/year={year}/month={month}/day={day}\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
