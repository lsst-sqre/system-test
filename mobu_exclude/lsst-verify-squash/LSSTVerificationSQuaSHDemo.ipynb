{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of the LSST verification framework and SQuaSH \n",
    "\n",
    "**Authors**: Angelo Fausti\n",
    "\n",
    "**Last Update**: Jun, 12 2019\n",
    "\n",
    "This notebook shows how to use the [LSST verification framework](https://pipelines.lsst.io/modules/lsst.verify/index.html) to create a new metric and track metric values in SQuaSH. In the example, we'll track an hypothetical metric, the *average camera body surface temperature* over time. For additional information, please refer to [LSST Verification Framework API Demonstration](https://sqr-019.lsst.io) and [The SQuaSH metrics dashboard](https://sqr-009.lsst.io/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import getpass\n",
    "import requests\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "\n",
    "import lsst.verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the LSST verification framework (`lsst.verify`), a metric is a measurable quantity that can be tracked. \n",
    "\n",
    "Pipelines metrics are defined in the [verify_metrics](https://github.com/lsst/verify_metrics) metrics package.  For example, [verify_metrics/metrics/validate_drp.yaml](https://github.com/lsst/verify_metrics/blob/master/metrics/validate_drp.yaml) contains definitions for all the metrics measured by `validate_drp`.\n",
    "\n",
    "In this demonstration, we will create an `example` metrics package to define our metric instead of using `verify_metrics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric has [name, description, unit, references, and tags](https://sqr-019.lsst.io/#Defining-metrics). Our metric is measured by an hypothetical `camera` package and is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat example/metrics/camera.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric name is prefixed by the package that measures the metric, in our example `camera.AvgCameraBodySurfaceTemp`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `lsst.verify.MetricSet` object with the metric definiton: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = lsst.verify.MetricSet.load_metrics_package(\"./example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = lsst.verify.MetricSet.load_metrics_package(\"./example\", subset='camera')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_task = lsst.verify.MetricSet.load_metrics_package(\"./example\", subset='my_task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicify, let's generate an abitrary value for our metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.random.normal(10, 0.1, 1000)\n",
    "avg_temp = np.mean(temp) * u.deg_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To captutre `avg_temp` as the metric value, we create a `lsst.verify.Measurement` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_temp_meas = lsst.verify.Measurement('camera.AvgCameraBodySurfaceTemp', avg_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a verification job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `lsst.verify` a “verification job” represents a pipeline run that measure metrics. The metric values are packaged in a `lsst.verify.Job` object. With a `lsst.verify.Job` object, we can then analyze the metric values, save them to disk, and dispatch to SQuaSH. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_job = lsst.verify.Job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we insert the metric value into the verification job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_job.measurements.insert(avg_temp_meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to add metadata about the execution environment, the task configuration or any useful information to analyze the metric values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_job.meta.update({'camera_name': 'ComCam'})\n",
    "camera_job.meta.update({'number_of_ccds': 9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add the metric definition to the verification job, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_job.metrics.update(camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Serialization to disk is a shim until we can persist with the butler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_job.write('camera.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cat camera.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispatching verification jobs to SQuaSH via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQuaSH supports two execution environments our Jenkins CI and the LDF. Dispatching verification jobs to SQuaSH is currently automated in these environments. A DM developer might want to dispatch verification jobs to SQuaSH manually, from a local execution environment. This capability is not fully supported yet (see [QAWG-REC-37](https://jira.lsstcorp.org/browse/DM-18057)). However, we can demonstrate how it works using a sandbox instance of SQuaSH specially deployed for this purpose:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is one time setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUASH_API_URL = \"https://squash-restful-api-sandbox.lsst.codes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only authenticated users can dispatch verification jobs to SQuaSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = getpass.getuser()\n",
    "password = getpass.getpass(prompt='Password for user `{}`: '.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, we can use the SQuaSH `register` API to register a new user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/register'.format(SQUASH_API_URL), json={'username': username, 'password': password})\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/auth'.format(SQUASH_API_URL), json={'username': username, 'password': password})\n",
    "access_token = r.json()['access_token']\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the SQuaSH `metrics` API to upload the metric definition to the SQuaSH.  This is a one time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/metrics'.format(SQUASH_API_URL), json={'metrics': camera.json}, headers={'Authorization': 'JWT {}'.format(r.json()['access_token'])})\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now dispatch the job to SQuaSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQuaSH requires some additional metadata, in [DM-18057](https://jira.lsstcorp.org/browse/DM-18057) we want to make this more flexible: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_job.meta.update({'packages': []})\n",
    "camera_job.meta.update({'env': {'env_name': 'jenkins'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we dispatch the verification job to SQuaSH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_job.dispatch(api_user=username, api_password=password, api_url=SQUASH_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling jobs at run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTask:\n",
    "    def __init__(self, run_name='default'):\n",
    "        my_task = lsst.verify.MetricSet.load_metrics_package(\"./example\", subset='my_task')\n",
    "        self.job = lsst.verify.Job()\n",
    "        self.job.meta.update({'task name': 'MyTask'})\n",
    "        self.job.meta.update({'run_name': run_name})\n",
    "        self.job.metrics.update(my_task)\n",
    "    def run(self):\n",
    "        val = np.random.random(1)[0]*500.\n",
    "        val_meas = lsst.verify.Measurement('my_task.MySuperMetric1', val)\n",
    "        self.job.measurements.insert(val_meas)\n",
    "    def finish(self):\n",
    "        meas = lsst.verify.Measurement('my_task.MySuperMetric2', u.Quantity(id(self),u.ct))\n",
    "        self.job.measurements.insert(meas)\n",
    "        self.job.write('my_task.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtask = MyTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtask.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtask.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "more my_task.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is needed the first time a metric is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/metrics'.format(SQUASH_API_URL), json={'metrics': my_task.json}, headers={'Authorization': 'JWT {}'.format(access_token)})\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the file to SQuaSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "      dispatch_verify.py \\\n",
    "        --env jenkins \\\n",
    "        --ignore-lsstsw \\\n",
    "        --url https://squash-restful-api-sandbox.lsst.codes \\\n",
    "        --user krughoff \\\n",
    "        --password $MY_SQUASH_PASSWD \\\n",
    "        my_task.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing metrics in Chronograf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a verification job is dispatched to SQuaSH, the metric values and the associated metadata are stored in InfluxDB, a time-series database. [Chronograf](https://chronograf-demo.lsst.codes) is the user interface for querying and visualizing InfluxDB time-series data.\n",
    "\n",
    "**NOTE**: The verification jobs sent to the SQuaSH sandbox instance are stored in the `squash-sandbox` InfluxDB database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Chronograf UI](chronograf-ui.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    " 1. Add specifications\n",
    " 2. Create alert rules using Kapacitor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
