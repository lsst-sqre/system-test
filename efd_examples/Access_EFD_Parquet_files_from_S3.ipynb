{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acessing EFD data stored as Parquet files from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angelo Fausti, Simon Krughoff\n",
    "\n",
    "November 1, 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The EFD uses the [Amazon S3 Sink connector](https://docs.confluent.io/kafka-connect-s3-sink/current/overview.html) to convert data from Kafka topics in Avro format to Parquet format with snappy compression. \n",
    "In this notebook we show how to access EFD data stored as Parquet files from S3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Parquet files from S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon S3 Sink connector runs at LDF, EFD data is replicated from the Summit and stored in an S3 bucket: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"efd-int\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AWA credentials to read this bucket can be found in 1Password (search for \"EFD AWS S3 credentials\" and then \"Credentials for the efd-reader-s3 IAM user\"). \n",
    "Add the read credentials to the `~/.aws/credentials` file.\n",
    "\n",
    "For example:\n",
    "```\n",
    "cat ~/.aws/credentials\n",
    "[default]\n",
    "aws_access_key_id = <the aws_access_key_id>\n",
    "aws_secret_access_key = <the aws_secret_access_key>\n",
    "```\n",
    "\n",
    "The S3 region can be added to the `~/.aws/config` file.\n",
    "\n",
    "```\n",
    "cat ~/.aws/config \n",
    "[default]\n",
    "region=us-east-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connector  is configured to partition data from Kafka topics by time on an hourly basis using the `Record` timestamp (added by Kafka when the message arrived the kafka broker) as reference. \n",
    "The following helps to construct the path to find the Parquet files on S3 for one of the topics, for example, `lsst.sal.ATCamera.logevent_heartbeat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"lsst.sal.ATCamera.logevent_heartbeat\"\n",
    "year = \"2021\"\n",
    "month = \"10\"\n",
    "day = \"28\"\n",
    "hour = \"01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `bucket.download_fileobj()` method to download the Parquet files into a buffer, and then Pyarrow to read the files, convert and append them to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for obj in bucket.objects.filter(Prefix=f\"topics/{topic}/year={year}/month={month}/day={day}/hour={hour}\"):\n",
    "    buffer = io.BytesIO()\n",
    "    bucket.download_fileobj(obj.key, buffer)\n",
    "    df = df.append(pq.read_table(buffer).to_pandas())\n",
    "    print(f\"{bucket.name}:{obj.key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connector is configured to invoke file commits to S3 every 10 minutes (see the [`rotate.interval.ms` configuration setting](https://docs.confluent.io/kafka-connect-s3-sink/current/configuration_options.html)), so you should see 6 files in this path.\n",
    "\n",
    "NOTE: To read all the Parquet files on a given day you can filter the bucket objects using a prefix like `f\"topics/{topic}/year={year}/month={month}/day={day}\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
