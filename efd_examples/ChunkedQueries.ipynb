{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunked queries: Efficient Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with large result sets, fetching the entire data at once can lead to excessive memory usage and slower performance. \n",
    "Fortunately, there is a solution called \"chunked queries\" that allows us to retrieve data in smaller, manageable chunks. \n",
    "By employing this technique, we can optimize memory usage and significantly improve query performance.\n",
    "\n",
    "Chunked queries are particularly useful when working with datasets that contain millions of data points. \n",
    "Rather than requesting the entire result set in one go, we can specify a maximum chunk size to split the data into smaller portions. \n",
    "\n",
    "It's important to note that the optimal chunk size may vary depending on the specific query.\n",
    "While it may seem intuitive that a smaller chunk size would result in faster query execution, that's not always the case. In fact, setting the chunk size too small can introduce overhead by generating a large number of requests to the database. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lsst_efd_client import EfdClient\n",
    "client = EfdClient('usdf_efd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable chunked responses in your query, you can simply set the chunked parameter to `True`. \n",
    "Additionally, you can specify the `chunk_size` parameter, which determines the maximum number of data points in each chunk. \n",
    "By default, the chunk size is set to 1000, but you can adjust it based on your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T16:07:53.137958Z",
     "iopub.status.busy": "2023-07-06T16:07:53.137446Z",
     "iopub.status.idle": "2023-07-06T16:07:53.141417Z",
     "shell.execute_reply": "2023-07-06T16:07:53.140923Z",
     "shell.execute_reply.started": "2023-07-06T16:07:53.137939Z"
    },
    "tags": []
   },
   "source": [
    "Let's consider an example where we need to query the `lsst.sal.MTM1M3.forceActuatorData` topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "await client.get_schema(\"lsst.sal.MTM1M3.forceActuatorData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T16:19:42.348212Z",
     "iopub.status.busy": "2023-07-06T16:19:42.347824Z",
     "iopub.status.idle": "2023-07-06T16:19:42.352456Z",
     "shell.execute_reply": "2023-07-06T16:19:42.351791Z",
     "shell.execute_reply.started": "2023-07-06T16:19:42.348193Z"
    },
    "tags": []
   },
   "source": [
    "\n",
    "This particular topic contains a massive payload, making it inefficient to retrieve all the data using a `SELECT * FROM ...` statement. \n",
    "Instead, it is recommended to select only the fields of interest. \n",
    "This significantly improves the query speed.\n",
    "\n",
    "The topic schema reveals the presence of array fields. \n",
    "In this topic, each element within the array corresponds to a distinct force actuator. \n",
    "Specifically, there are 156 actuators responsible for exerting forces in the `x` direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fields =  \", \".join([f\"xForce{i}\" for i in range(156)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f'''SELECT {fields} FROM \"lsst.sal.MTM1M3.forceActuatorData\" WHERE time > now()-1h'''\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T16:10:04.755017Z",
     "iopub.status.busy": "2023-07-06T16:10:04.754690Z",
     "iopub.status.idle": "2023-07-06T16:10:04.758705Z",
     "shell.execute_reply": "2023-07-06T16:10:04.758191Z",
     "shell.execute_reply.started": "2023-07-06T16:10:04.754995Z"
    },
    "tags": []
   },
   "source": [
    "By implementing chunked queries with the appropriate configuration, we can retrieve a dataframe with approximately 30 million data points in less than a minute.\n",
    "\n",
    "This approach leverages the power of chunked responses and targeted data selection to achieve efficient and timely data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = await client.influx_client.query(query, chunked=True, chunk_size=1000)\n",
    "df = pd.concat([chunk async for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
